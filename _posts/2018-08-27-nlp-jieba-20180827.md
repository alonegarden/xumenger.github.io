---
layout: post
title: 自然语言处理入坑记 
categories: 人工智能相关 自然语言处理 
tags: 人工智能 知识图谱 KG 自然语言处理 爬虫 jieba NLP 中文 MIT 
---

>紧接着[知识图谱入坑记](http://www.xumenger.com/knowledge-graph-20180820/)

[jieba](https://github.com/fxsjy/jieba) 可以说是现在做的最好的开源版本的中文分词组件！

按照官方的说法，jieba 具有这样的一些特点

* 支持三种分词模式
	* 精确模式：试图将句子最精确地切开，适合文本分析
	* 全模式：把句子中所有可以成词的词语都扫描出来，速度快但不能解决歧义
	* 搜索引擎模式：在精确模式基础上，对长词再切分，提高召回率，适合于搜索引擎分词
* 支持繁体分词
* 支持自定义词典
* MIT授权协议

## 分词模式展示

先直接上效果图！

![](../media/image/2018-08-27/01.png)

对应的代码是这样的

```python
import jieba

seg_list = jieba.cut("我从今天开始学习自然语言处理", cut_all=True)
print("全模式: " + "/ ".join(seg_list))

seg_list = jieba.cut("我从今天开始学习自然语言处理", cut_all=False)
print("精确模式: " + "/ ".join(seg_list))

seg_list = jieba.cut_for_search("我从今天开始学习自然语言处理，一门全新的技术")
print("搜索引擎模式: " + "/ ".join(seg_list))
```

