---
layout: post
title: 调试与调优：网络相关
categories: 好资源之开发小纸条 java之内存管理 深入学习之编译原理 c/c++之指针与内存 深入学习之逆向工程
tags: C C++ Java JVM MySQL Oracle 网络 内存 网络 IO CPU 线程 GC FullGC Spring SpringBoot Kafka Redis ElasticSearch Kafka 虚拟机与指令集 算法与数据结构 
---

![](../media/image/2020-09-24/03-01.png)

![](../media/image/2020-09-24/03-02.png)

## TIME_WAIT 状态连接过多

并发请求量增大之后，继续发起请求，发现开始大量报错，主要报TCP 连接异常的错误

```
netstat -nat | grep 192.168.0.1 | grep TIME_WAIT | wc -l
```

统计发现有15000 个TIME_WAIT 状态的连接

因为系统短时间内大量的并发请求导致大量TCP 连接，且系统没有及时回收和重用TIME_WAIT 的连接，导致TIME_WAIT 的连接不断增加，系统无法及时分配可用的连接

修改Linux 系统参数，开始TIME-WAIT 状态的socket 重用和快速回收

```
net.ipv4.tcp_syncookies = 1        // 开启SYN cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭
net.ipv4.tcp_tw_reuse = 1          // 开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭
net.ipv4.tcp_tw_recycle = 1        // 开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭
net.ipv4.tcp_fin_timeout = 30      // 修改系統默认的 TIMEOUT 时间
```

然后执行 /sbin/sysctl -p 让参数生效

## CLOSE_WAIT 状态过多

应用服务器报错：too many open files

```
lsof -n | awk '{print $2}' | sort | uniq -c | sort -nr | more 
```

查看各进程打开的文件句柄情况，第一列是打开的文件数，第二列是进程ID，发现报错的这个应用服务器的句柄数是60000多

```
lsof | grep <pid> | grep CLOSE_WAIT | wc -l
```

统计发现主要是CLOSE_WAIT 状态TCP 连接

大量CLOSE_WAIT 的原因是，客户端调用close() 关闭了连接，但服务端没有进行close()

代码需要判断socket，一旦read() 到0，就需要close()，如果read() 返回负，检查errno 如果不是AGAIN，也close()

也有的是因为没有在finally 里面进行close，导致出现异常的时候没有正确关闭socket 连接，从而出现这种问题

## 端口数限制导致TCP 连接数到达上限

应用的架构是这样的，客户端 -> 路由服务器 -> 应用服务器集群 -> 数据库服务器

发起压测后，TPS达到600之后就上不去了

逐个环节排查

* 检查数据库最大连接数和当前连接数，可知数据库的连接数也不是瓶颈，判断得瓶颈不在应用服务器和数据库服务器上
* 检查应用服务器的线程栈，发现不是应用服务器的瓶颈，不通过路由服务器，直接对应用服务器集群压测可以达到1000多TPS
* 观察路由服务器的CPU、线程数、网络连接数，发现其TCP当前连接数很大，达到25000以上，但是后续不再增加，且大部分都是处理TIME_WAIT 状态

针对TIME_WAIT 的问题，参考第一个的修改方案

TCP 连接数达到25000 之后，不再增加，判断可能是TCP 端口瓶颈导致的

修改net.ipv4.tcp_local_port_range 的配置，默认值是32768~60999，修改为10000~60999

net.ipv4.tcp_max_buckets 从32768 修改为20000，在TIME_WAIT 数量等于这个值时，不会有新的TIME_WAIT 产生
 
