---
layout: post
title: 搭建Spark 环境
categories: 大数据之kafka 大数据之spark
tags: scala java 大数据 kafka spark MacOS 环境搭建 Scala Maven Hadoop 
---

## 新建一个Scala 项目

在Scala IDE 中新建一个Maven 应用

![](../media/image/2020-11-23/01-01.png)

![](../media/image/2020-11-23/01-02.png)

此时创建的工程还不是Scala 的工程，在项目名称上-->点击右键-->Configure-->Add Scala Nature

点击Add Scala Nature 后项目才转换为Scala-Maven 项目

>推荐[eclipse + maven + scala + spark环境搭建](https://www.cnblogs.com/wmm15738807386/p/6723391.html)

编写测试代码

```scala
package com.xum

object HelloSpark {
  def main(args: Array[String]): Unit = {
    println("HelloWorld")
  }
}
```

运行效果如下

![](../media/image/2020-11-23/01-03.png)

下面的案例需要依赖Spark-Core，这里先加好依赖

```xml
  <dependencies>
    <dependency>
      <groupId>org.apache.spark</groupId>  
      <artifactId>spark-core_2.12</artifactId>  
      <version>3.0.0</version>  
    </dependency>
  </dependencies>
```

这时候，点击项目-->Run As--> Maven Test

## 搭建Spark 环境

进入官网下载页面[https://archive.apache.org/dist/spark/spark-3.0.0/](https://archive.apache.org/dist/spark/spark-3.0.0/)

选择下载spark-3.0.0-bin-hadoop3.2.tgz，这样就不用事先安装Hadoop 了

下载完成后，放到指定目录，然后按照下面的步骤操作

```shell
# 解压缩该文件
sudo tar -zvxf spark-3.0.0-bin-hadoop3.2.tgz

# 配置环境变量
sudo vim ~/.bash_profile

# 在配置文件里添加如下配置后保存退出
export SPARK_HOME="/Users/xumenger/Desktop/library/spark-3.0.0-bin-hadoop3.2"
export PATH=$PATH:$SPARK_HOME/bin

# 配置文件生效
source ~/.bash_profile
```

进入Spark 的目录中的sbin 子目录下，启动Spark：./start-all.sh

![](../media/image/2020-11-23/02-01.png)

进去Spark目录下的bin子目录，使用Spark Shell 进行测试：./spark-shell

![](../media/image/2020-11-23/02-02.png)

准备一个测试用的文件，内容如下

```
Hello Spark
Hello Scala
```

然后通过在Spark 运行环境下对其进行单词统计工作：

```
sc.textFile("/Users/xumenger/Desktop/code/Spark/data/1.txt").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).collect
```

![](../media/image/2020-11-23/02-03.png)

浏览器输入[http://localhost:4040/jobs/](http://localhost:4040/jobs/) 可以看到刚才执行的这个Job。刚才提交的是一个collect 方法，可以看到其提交时间、执行时间等信息

![](../media/image/2020-11-23/02-04.png)

依次点击进去可以看到更详细的信息

![](../media/image/2020-11-23/02-05.png)

![](../media/image/2020-11-23/02-06.png)

## 提交任务到Spark 环境运行

在IDEA 或Eclipse 中开发的应用打包为一个jar 包，然后提交到这个环境中可以执行，比如Spark 自己在服务端运行环境下./examples/jars/ 中有一个spark-examples_2.12-3.0.0.jar的案例

![](../media/image/2020-11-23/02-07.png)

然后执行下面的命令即可以提交这个任务执行

```
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

![](../media/image/2020-11-23/02-08.png)

>这个是一个local 本地模式的演示，在真实工作中还是要将应用提交到集群中去执行的！

Spark 有各种集群模式，比如其中Spark 自身节点运行的集群模式，即所谓的独立部署（Standalone）模式，使用了经典的master-slave 模式

机器         | Linux1       | Linux2        | Linux3
------------ | ------------ | -------------| -------------
角色        | Worker Master | Worker        | Worker

## Word Count 代码实现

>全部参考[尚硅谷2020最新版大数据Spark从入门到精通](https://www.bilibili.com/video/BV11A411L7CK?p=5)

![](../media/image/2020-11-23/03-01.png)

再使用聚合的思路优化一下上面的分析过程，拆分之后顺便记一下每个单词的数量为1，再基于这个拆分的结果进行后续处理

![](../media/image/2020-11-23/03-02.png)

编写代码如下

```scala
object HelloSpark {
  def main(args: Array[String]): Unit = {
    // 创建和Spark 框架的连接
    val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")
    val sc = new SparkContext(sparkConf)

    // 编写业务逻辑

    // 关闭连接
    sc.stop()

  }
}
```

运行程序后输出信息如下

![](../media/image/2020-11-23/03-03.png)