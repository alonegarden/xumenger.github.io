---
layout: post
title: Spark 计算框架：Spark Core
categories: 大数据之kafka 大数据之spark
tags: scala java 大数据 kafka spark MacOS 环境搭建 Scala Maven Hadoop RDD 累加器 广播变量 装饰者 设计模式 IO 
---

Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景，分别是

* RDD：弹性分布式数据集
* 累加器：分布式共享只写变量
* 广播变量：分布式共享只读变量

## 什么是RDD？

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark 中最基本的数据处理模型。在代码中是一个抽象类，它代表了一个弹性的、不可变的、可分区的、里面的元素可并行计算的集合

比如，要实现一个逻辑，如果逻辑很复杂，那么后面扩展起来相对就不是很方便，所以把RDD 封装为最小计算单元，如果有新的逻辑的话，，在原来的基础上做一个关联就好了。所以在程序中可能不止一个RDD，每个RDD 都是一个计算单元，把多个RDD 关联在一起就会形成一个复杂的逻辑

这个复杂的逻辑从Driver 传给Executor 就可以完成复杂的计算需求了

![](../media/image/2020-11-25/01.png)

RDD 简单的理解成把数据准备好，把逻辑准备好的一种结构

>[https://www.bilibili.com/video/BV11A411L7CK?p=26](https://www.bilibili.com/video/BV11A411L7CK?p=26)

Spark 中的RDD 在理解上可以和Java 的IO 流进行**类比**，比如下图使用FileInputStream 处理字节流，然后InputStreamReader 转换成字符流，再给到BufferedReader 处理

![](../media/image/2020-11-25/02.png)

基于装饰者设计模式，每个RDD 按照自己的逻辑处理数据，然后将处理得到的结果给到下一个（或者叫下一层）RDD

![](../media/image/2020-11-25/03.png)

RDD 的数据只有在调用collect 方法时，才会真正执行业务逻辑操作，之前的封装全部都是功能的扩展。不同于IO 临时保存一部分数据，RDD 是不保存数据的，上面的图中的数据只是展示计算的步骤用的！

下面是RDD 源码中的注释说明

```java
 /**
 * A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,
 * partitioned collection of elements that can be operated on in parallel. This class contains the
 * basic operations available on all RDDs, such as `map`, `filter`, and `persist`. In addition,
 * [[org.apache.spark.rdd.PairRDDFunctions]] contains operations available only on RDDs of key-value
 * pairs, such as `groupByKey` and `join`;
 * [[org.apache.spark.rdd.DoubleRDDFunctions]] contains operations available only on RDDs of
 * Doubles; and
 * [[org.apache.spark.rdd.SequenceFileRDDFunctions]] contains operations available on RDDs that
 * can be saved as SequenceFiles.
 * All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)])
 * through implicit.
 *
 * Internally, each RDD is characterized by five main properties:
 *
 *  - A list of partitions
 *  - A function for computing each split
 *  - A list of dependencies on other RDDs
 *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
 *    an HDFS file)
 *
 * All of the scheduling and execution in Spark is done based on these methods, allowing each RDD
 * to implement its own way of computing itself. Indeed, users can implement custom RDDs (e.g. for
 * reading data from a new storage system) by overriding these functions. Please refer to the
 * <a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf">Spark paper</a>
 * for more details on RDD internals.
 */
```
