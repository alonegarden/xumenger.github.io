---
layout: post
title: Spark 计算框架：Spark SQL
categories: 大数据之kafka 大数据之spark
tags: scala java 大数据 kafka spark MacOS 环境搭建 Scala Maven Hadoop SQL Hive 关系型数据库 结构化数据 RDD RDMS 数仓 DataFrame DataSet
---

Spark SQL 就是对结构化数据处理的模块。为什么会出现Spark SQL 呢？先说一下Hive，Hive 是基于Hadoop 框架的SQL 引擎工具，目的是简化Map Reduce 开发，因为早期很多大数据开发人员都来自Web 开发或者Server 开发，这些程序员对于大数据开发不是很理解，所以开发效率并不高，因此Hive 就出现了，它用SQL 的方式简化Map Reduce，也就是说开发者编写一个SQL，框架把它转换成Map Reduce执行，执行效率可能不会提高，但是开发效率可以有效提升。Spark SQL 的出现则是为了简化RDD 的开发！

Spark SQL 为了简化RDD 的开发，提高开发效率，提供了2 个抽象模型，分别是DataFrame、DataSet

工作中会有数仓相关的业务，会和Hive 相结合，Spark SQL 在这个场景下会是一个不错的选择！

Spark SQL 使用相同的方式可以连接不同的数据源，比如数据库、HBase、Hive、文件等，只需要简单地修改一下参数即可

## DataFrame与DataSet

在Spark 中，DataFrame 是一种以RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与RDD 的主要区别在于，前者带有schema 元信息，即DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL 得以洞察更多的结构信息，从而对藏于DataFrame 背后的数据源以及作用于DataFrame 之上的变换进行了针对性的优化，最终达到大幅度提升运行时效率的目标

反观RDD，由于无法得知所存数据元素的具体内部结构，Spark Core 只能在Stage 层面进行简单、通用的流水线优化

与Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和map），从API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的RDD API 要更加友好

![](../media/image/2020-11-26-2/01.png)

DataFrame 是为数据提供了Schema 的视图，可以把它当作数据库中的一张表来对待。DataFrame 也是懒执行的，但性能上比RDD 高，主要原因是优化的执行计划，即查询计划通过Spark catalyst optimiser 进行优化

DataSet 是分布式数据集合。DataSet 是Spark 1.6 中添加的一个新抽象，是DataFrame 的一个扩展，它提供了RDD 的优势（强类型、使用强大的lambda 函数的能力）以及Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作map、flatMap、filter等）

## 控制台环境演示DataFrame

准备一个JSON 格式的结构化数据

```json
{"name":"xumenger", "age":18, "address":"xuzhou"}
{"name":"joker", "age":38, "address":"hangzhou"}
```

然后展示在控制台环境下的应用（注意：在控制台环境下，sc 指的是SparkContext 实例，spark 是指SparkSession 实例）

![](../media/image/2020-11-26-2/02.png)

比如想要计算杭州的用户的年龄的平均值，很简单

```sql
select avg(age) 
from user
where address = "hangzhou"
```

试想一下如果用RDD 实现这个功能，那么相对上面的方式会复杂很多，所以Spark SQL 是很好的简化RDD 的方式

>[https://www.bilibili.com/video/BV11A411L7CK?p=157](https://www.bilibili.com/video/BV11A411L7CK?p=157)

>[https://www.bilibili.com/video/BV11A411L7CK?p=158](https://www.bilibili.com/video/BV11A411L7CK?p=158)

## RDD、DataFrame转换

同样，先在控制台环境下展示

![](../media/image/2020-11-26-2/03.png)

调用toDF()，并且在toDF() 的参数中指定列名，即可从RDD 转化为DataFrame。另外DataFrame 也可以转化为RDD

![](../media/image/2020-11-26-2/04.png)

## DataSet 演示

DataSet 是具有强类型的数据集合，需要提供对应的类型信息

![](../media/image/2020-11-26-2/05.png)

关于RDD、DataFrame、DataSet 之间的关系可以用下图来进行理解

![](../media/image/2020-11-26-2/06.png)

## DataFrame 与DataSet相互转换

上面看到了RDD、DataFrame、DataSet 之间的关系，也讲到了RDD 与DataFrame 可以相互转换，DataFrame 与DataSet 之间也是可以相互转换的

![](../media/image/2020-11-26-2/07.png)

在后期的Spark 版本中，DataSet 有可能逐步取代RDD 和DataFrame，成为唯一的数据结构！

## 在Eclipse 中基于Spark SQL 开发

首先需要引入必要的Maven 依赖

```xml
<dependency>
  <groupId>org.apache.spark</groupId>  
  <artifactId>spark-sql_2.12</artifactId>  
  <version>3.0.0</version>  
</dependency>
```

直接来实现一个简单的例子

```scala
package com.xum.sql

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD

object SparkSQLExample 
{
  def main(args: Array[String]): Unit = {
    // 创建Spark 运行配置对象，连接
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQLExample")
    
    //创建 SparkSession 对象
    val sparkSession: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate() 
    
    //读取 json 文件 创建 DataFrame 
    val df = sparkSession.read.json("/Users/xumenger/Desktop/code/Spark/data/user.json")
    df.show()
    
    // SQL 风格语法
    df.createOrReplaceGlobalTempView("user")
    sparkSession.sql("select avg(age) from global_temp.user").show
    
    // DSL 风格语法
    df.select("name", "age").show
    
    // 创建一个RDD（地址、年龄、姓名）
    val rdd1 : RDD [(String, Int, String)] = sparkSession.sparkContext.makeRDD(List(("hangzhou", 18, "jk"), ("xuzhou", 19, "xumeng")))
    
    // 需要这句，否则会报错：value toDF is not a member of org.apache.spark.rdd.RDD[(String, Int, String)]
    import sparkSession.implicits._
    // RDD 转换成DataFrame
    val rdd2DF = rdd1.toDF("address", "age", "name")
    rdd2DF.show
    
    // DataFrame转换成DataSet，指定类型为case class User(address: String, age: Long, username: String)
    val df2DS = rdd2DF.as[User]
    df2DS.show
    
    // DataSet 转换成DataFrame
    val ds2DF = df2DS.toDF()
    ds2DF.show
    
    // DataSet 转换成RDD
    val ds2RDD = df2DS.rdd
    ds2RDD.foreach(println)
    
    // RDD 转换成DataSet
    val rdd2DS = rdd1.map{
      case (address, age, name) => User(address, age, name)
    }.toDS().show
    
    // 释放资源
    sparkSession.stop()
  }
}

case class User(address: String, age: Long, name: String)
```
